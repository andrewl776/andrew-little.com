blogdown::install_theme("wowchemy/wowchemy-hugo-themes", force = T, theme_example = TRUE)
warnings()
blogdown::build_site()
blogdown::check_site()
blogdown::install_theme("kakawait/hugo-tranquilpeak-theme")
blogdown::build_site()
blogdown::build_site()
blogdown::build_site()
blogdown::check_site()
blogdown::check_site()
blogdown::build_site()
blogdown::build_site()
blogdown::check_site()
blogdown::build_site()
blogdown::build_site()
blogdown::check_site()
blogdown::check_site()
blogdown::build_site()
blogdown::check_site()
blogdown::build_site()
blogdown::serve_site()
blogdown::build_site()
blogdown::clean_duplicates()
blogdown::check_config()
blogdown::check_content()
blogdown::serve_site()
blogdown::stop_server()
blogdown::serve_site()
blogdown::build_site()
blogdown::build_site()
blogdown::check_site()
blogdown::serve_site()
blogdown::build_site(local = TRUE)
blogdown::serve_site()
blogdown::stop_server()
blogdown::serve_site()
blogdown::build_site()
blogdown::serve_site()
blogdown::stop_server()
blogdown::serve_site()
blogdown::stop_server()
blogdown::check_site()
blogdown::check_site()
blogdown::build_site()
blogdown::serve_site()
blogdown::stop_server()
blogdown::serve_site()
blogdown::stop_server()
blogdown::serve_site()
library(blogdown)
serve_site()
stop_server()
build_site()
serve_site()
stop_server()
build_site()
serve_site()
serve_site()
stop_server()
serve_site()
build_site()
serve_site()
stop_server()
stop_server()
serve_site()
stop_server()
serve_site()
build_site()
build_site()
serve_site()
stop_server()
serve_site()
stop_server()
serve_site()
stop_server()
serve_site()
build_site()
build_site()
build_site()
stop_server()
serve_site()
library(lubridate)
library(tidyverse)
library(rvest)
library(spotifyr)
library(stringi)
library(glue)
library(progress)
library(tidytext)
library(textdata)
library(wordcloud)
library(wordcloud2)
library(reshape2)
library(topicmodels)
library(furrr)
library(LDAvis)
library(SnowballC)
library(stopwords)
library(parallel)
library(lexicon)
library(sentimentr)
library(magrittr)
library(tidymodels)
library(textrecipes)
library(discrim)
library(lsa)
library(naivebayes)
source("utils.R")
library(lubridate)
library(tidyverse)
library(rvest)
library(spotifyr)
library(stringi)
library(glue)
library(progress)
library(tidytext)
library(textdata)
library(wordcloud)
library(wordcloud2)
library(reshape2)
library(topicmodels)
library(furrr)
library(LDAvis)
library(SnowballC)
library(stopwords)
library(parallel)
library(lexicon)
library(sentimentr)
library(magrittr)
library(tidymodels)
library(textrecipes)
library(discrim)
library(lsa)
library(naivebayes)
source("utils.R")
# 1) Scrape chart data ----
start_date <- mdy("12/29/2016")
stop_date <- mdy("09/02/2021")
# Define what dates (and therefore URLs) we will iterate over
dates <- tibble("from" = seq(start_date, stop_date, by = "weeks")) %>%
mutate(from = from + days(1),
"to" = lead(from)) %>%
drop_na() %>%
mutate("url" = paste0("https://spotifycharts.com/regional/global/weekly/",
from, "--", to))
# Iterate over URLs and bind rows together (in parallel)
# Create cluster
cl <- parallel::makeCluster(4)
# Give each core the "dates" object
parallel::clusterExport(
cl,
"dates"
)
# Library in the tidyverse and rvest once in each cluster
parallel::clusterEvalQ(cl, {
library(tidyverse)
library(rvest)
})
# Loop in parallel
chart_data <- parallel::parLapply(cl, dates$url, scrape_chart_data)
# Stop cluster
parallel::stopCluster(cl)
# Let's have a look at how this function works
scrape_chart_data(dates$url[1])
# Bind rows together and only pick out distinct tracks
chart_data <- chart_data %>%
bind_rows() %>%
distinct(track_name, artist, .keep_all = TRUE)
chart_data
# EXERCISE: See slides
# Here is one we made earlier
chart_data <- readr::read_csv("data/output/chart_data.csv.gz")
# 2) Enrich dataset with spotify API ----
# https://developer.spotify.com/documentation/web-api/reference/
# If you're trying to follow along, don't worry about setting up Spotify credentials,
# use the dataset provided.
# IMPORTANT: need to set environment variables below using API credentials
# https://developer.spotify.com/my-applications/#!/applications
# -> use spotify account to log in, create application, credentials will be shown after
#
# Sys.setenv(SPOTIFY_CLIENT_ID = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')
# Sys.setenv(SPOTIFY_CLIENT_SECRET = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')
#
# Confirm everything is working with this function:
# get_spotify_access_token()
# We will now enrich the dataset with tracks from specific playlists,
# as opposed to those at the top of the charts
# Search for playlists using keyword
playlists <- search_spotify("rock", type = "playlist", limit = 50)
usethis::use_github_actions()
system.file("README.md", package = "aRbs")
readr::read_file_raw("https://raw.githubusercontent.com/andrewl776/FPLdata/master/README.md")
readr::read_file("https://raw.githubusercontent.com/andrewl776/FPLdata/master/README.md")
readr::read_file("https://raw.githubusercontent.com/andrewl776/FPLdata/master/README.md") %>% writeLines("content/posts/FPLdata.md")
blogdown:::preview_site()
readLines("content/posts/readmes/template.yaml")
writeLines(readLines("content/posts/readmes/template.yaml"), "content/posts/readmes/template.yaml")
write(readLines("content/posts/readmes/template.yaml"), "content/posts/readmes/template.yaml", append = TRUE)
writeLines(readLines("content/posts/readmes/template.yaml"), "content/posts/readmes/FPLdata.md");
readLines("content/posts/readmes/template.yaml")
glue::glue(readLines("content/posts/readmes/template.yaml"))
template <- readLines("content/posts/readmes/template.yaml")
glue::glue(template)
glue::glue("{template}")
date <- Sys.Date()
glue::glue(glue::glue("{template}"), .open = "{{", .close = "}}")
glue::glue('{glue::glue("{template}")}', .open = "{{", .close = "}}")
glue::glue(lines = readLines("content/posts/readmes/template.yaml"))
glue::glue("{template}")
template
template
template %>% dput
glue::glue(c("---", "title: \"{FPLdata} R Package\"", "thumbnailImagePosition: left",
"thumbnailImage: //d1u9biwaxjngwg.cloudfront.net/cover-image-showcase/city-750.jpg",
"date: {{date}}", "categories:", "  - Packages", "- features",
"tags:", "  - pagination", "showPagination: false", "---", "",
""))
paste(template)
write_template <- function(date, title) {
template <- c("---", "title: \"{FPLdata} R Package\"", "thumbnailImagePosition: left",
"thumbnailImage: //d1u9biwaxjngwg.cloudfront.net/cover-image-showcase/city-750.jpg",
"date: {{date}}", "categories:", "  - Packages", "- features",
"tags:", "  - pagination", "showPagination: false", "---", "",
"")
vapply(template, function(s) glue::glue(s, .open = "{{", .close = "}}"), character(1))
}
write_template()
write_template(Sys.Date())
new_temp <- write_template(Sys.Date())
new_temp
write_template <- function(date) {
template <- c("---", "title: \"{FPLdata} R Package\"", "thumbnailImagePosition: left",
"thumbnailImage: //d1u9biwaxjngwg.cloudfront.net/cover-image-showcase/city-750.jpg",
"date: {{date}}", "categories:", "  - Packages", "- features",
"tags:", "  - pagination", "showPagination: false", "---", "",
"")
unname(vapply(template, function(s) glue::glue(s, .open = "{{", .close = "}}"), character(1)))
}
new_temp <- write_template(Sys.Date())
new_temp
blogdown::build_site()
blogdown::build_site()
blogdown::build_site()
blogdown::build_site()
blogdown::build_site()
blogdown::build_site()
blogdown::build_site()
blogdown::build_site()
blogdown::build_site()
blogdown::build_site()
blogdown::build_site()
library(blogdown)
serve_site()
stop_server()
serve_site()
stop_server()
serve_site()
blogdown::build_site()
stop_server()
serve_site()
stop_server()
blogdown::build_site()
serve_site()
blogdown::build_site()
stop_server()
serve_site()
stop_server()
serve_site()
fpl_data <- FPLdata()
library(FPLdata)
fpl_data <- FPLdata()
fpl_data
fpl_data <- FPLdata()
fpl_data
serve_site()
serve_site()
blogdown::build_site()
serve_site()
stop_server()
serve_site()
list.files("content/posts/readmes/")
paste0("content/posts/readmes/", list.files("content/posts/readmes/"))
blogdown::build_site()
library(blogdown)
serve:site()
serve_site()
blogdown::build_site()
serve_site()
stop_server()
serve_site()
serve_site()
blogdown::build_site()
serve_site()
serve_site()
blogdown::build_site()
serve_site()
blogdown::build_site()
serve_site()
stop_server()
serve_site()
stop_server()
blogdown::build_site()
serve_site()
stop_server()
blogdown::build_site()
serve_site()
stop_server()
blogdown::build_site()
Sys.Date()
blogdown:::preview_site()
stop_server()
blogdown::build_site()
blogdown:::preview_site()
blogdown::build_site()
stop_server()
blogdown::build_site()
blogdown::build_site()
stop_server()
serve_site()
blogdown::build_site()
blogdown::build_site()
serve_site()
stop_server()
serve_site()
stop_server()
serve_site()
library(lubridate)
library(tidyverse)
library(rvest)
library(spotifyr)
library(stringi)
library(glue)
library(progress)
library(tidytext)
library(textdata)
library(wordcloud)
library(wordcloud2)
library(reshape2)
library(topicmodels)
library(furrr)
library(LDAvis)
library(SnowballC)
library(stopwords)
library(parallel)
library(lexicon)
library(sentimentr)
library(magrittr)
library(tidymodels)
library(textrecipes)
library(discrim)
library(lsa)
library(naivebayes)
source("utils.R")
# 1) Scrape chart data ----
start_date <- mdy("12/29/2016")
stop_date <- mdy("09/02/2021")
# Define what dates (and therefore URLs) we will iterate over
dates <- tibble("from" = seq(start_date, stop_date, by = "weeks")) %>%
mutate(from = from + days(1),
"to" = lead(from)) %>%
drop_na() %>%
mutate("url" = paste0("https://spotifycharts.com/regional/global/weekly/",
from, "--", to))
# Iterate over URLs and bind rows together (in parallel)
# Create cluster
cl <- parallel::makeCluster(4)
# Give each core the "dates" object
parallel::clusterExport(
cl,
"dates"
)
# Library in the tidyverse and rvest once in each cluster
parallel::clusterEvalQ(cl, {
library(tidyverse)
library(rvest)
})
# Loop in parallel
chart_data <- parallel::parLapply(cl, dates$url, scrape_chart_data)
# Stop cluster
parallel::stopCluster(cl)
# Let's have a look at how this function works
scrape_chart_data(dates$url[1])
# Bind rows together and only pick out distinct tracks
chart_data <- chart_data %>%
bind_rows() %>%
distinct(track_name, artist, .keep_all = TRUE)
chart_data
# EXERCISE: See slides
# Here is one we made earlier
chart_data <- readr::read_csv("data/output/chart_data.csv.gz")
# 2) Enrich dataset with spotify API ----
# https://developer.spotify.com/documentation/web-api/reference/
# If you're trying to follow along, don't worry about setting up Spotify credentials,
# use the dataset provided.
# IMPORTANT: need to set environment variables below using API credentials
# https://developer.spotify.com/my-applications/#!/applications
# -> use spotify account to log in, create application, credentials will be shown after
#
# Sys.setenv(SPOTIFY_CLIENT_ID = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')
# Sys.setenv(SPOTIFY_CLIENT_SECRET = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')
#
# Confirm everything is working with this function:
# get_spotify_access_token()
# We will now enrich the dataset with tracks from specific playlists,
# as opposed to those at the top of the charts
# Search for playlists using keyword
playlists <- search_spotify("rock", type = "playlist", limit = 50)
get_spotify_access_token()
library(blogdown)
build_site()
blogdown::install_hugo("0.89.4")
build_site()
install.packages('gitcreds')
library(gitcreds)
gitcreds_set()
blogdown::check_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
build_site()
blogdown::serve_site()
stop_server()
stop_server()
blogdown::serve_site()
build_site()
build_site()
stop_server()
blogdown::serve_site()
install.packages('dlstats')
dlstats::cran_stats('FPLdata')
x <- dlstats::cran_stats('FPLdata')
x
sum(x$downloads)
x <- dlstats::cran_stats('aRbs')
x
sum(x$downloads)
build_site()
stop_server
stop_server()
serve_site()
stop_server()
build_site()
serve_site()
build_site()
stop_server()
serve_site()
stop_server()
serve_site()
stop_server()
build_site()
serve_site()
stop_server()
build_site()
serve_site()
serve_site()
stop_server()
serve_site()
stop_server()
build_site()
serve_site()
blogdown::serve_site()
library(bookdown)
check_site()
library(blogdown)
check_site()
build_site()
1+2
